---
title: "1. Computer Architecture Fundamentals, CPU Components, Memory Hierarchy, FPGAs"
author: "Zakhar Podyakov"
date: "September 16, 2025"
format: html
---

#### 1. Summary

##### 1.1 What is a Computer?
A **computer** is fundamentally a machine designed to perform sequences of arithmetic or logical operations automatically. Modern computers achieve this through a stored-program model, where an *electronic device operates under the control of a program stored in its memory*. At the most basic level, a computer takes **binary input** (data), manipulates it according to a **binary program** (a sequence of instructions), and produces **binary output** (a result). The two most essential components of any computer are the **Processor (CPU)**, which executes instructions, and the **Memory**, which stores both instructions and data.

##### 1.2 What is Computer Architecture?
**Computer Architecture** is the discipline that defines the conceptual design and fundamental operational structure of a computer system. It acts as the critical interface between the system's hardware and its software. Studying computer architecture involves understanding three key areas:

1.  **Hardware Organization**: The physical arrangement and interconnection of components like the CPU, memory, and storage.
2.  **Hardware/Software Interaction Principles**: The rules and methods by which software commands and controls the hardware, primarily through the **Instruction Set Architecture (ISA)**.
3.  **Performance-Related Aspects**: The analysis and design of systems to achieve high performance, considering factors like speed, power consumption, and efficiency.

##### 1.3 The Problem-Solution Stack
Solving a problem with a computer involves multiple layers of abstraction, from the physical world to the user's program. This stack illustrates how each layer builds upon the one below it, hiding complexity.

1.  **Problem to Solve**: The high-level goal (e.g., "render a 3D image").
2.  **Algorithm + Data Structures**: The logical method and data organization to solve the problem.
3.  **User Program**: The implementation of the algorithm in a high-level language (e.g., C++, Python).
4.  **System Programs**: The operating system and compilers that manage resources and translate the user program into machine-readable instructions.
5.  **Processor Instruction Set Architecture (ISA)**: The specific set of low-level commands the hardware can execute. This is the primary boundary between software and hardware.
6.  **Microarchitecture**: The specific implementation of the ISA in hardware, including how components like the ALU and registers are arranged.
7.  **Logic Circuits**: The fundamental building blocks (like AND/OR gates) that compose the microarchitecture.
8.  **Electrons/Photons**: The physical principles that make the logic circuits work.

##### 1.4 Core Hardware Components
A computer system is composed of several key hardware components that work together.

*   **CPU (Central Processing Unit)**: The "brain" of the computer, responsible for fetching, decoding, and executing program instructions. A CPU contains several key parts:
    *   **Control Unit (CU)**: Directs the flow of operations, fetching the next instruction from memory.
    *   **Arithmetic Logic Unit (ALU)**: Performs all arithmetic (e.g., addition, subtraction) and logic (e.g., AND, OR) operations.
    *   **Registers**: A small number of extremely fast memory locations located directly on the CPU chip. They hold data that is actively being used by the current instruction.

*   **The Memory Hierarchy**: Not all memory is created equal. To balance speed, cost, and capacity, computers use a tiered memory system. Accessing data is fastest at the top and becomes progressively slower—but larger and cheaper—at lower levels. The existence of this hierarchy is a solution to the *memory wall problem*, where CPUs can process data much faster than they can retrieve it from main memory.
    1.  **Registers** (Fastest, smallest, most expensive)
    2.  **CPU Cache** (L1, L2, L3)
    3.  **System Memory** (RAM)
    4.  **Storage Devices** (SSD, HDD) (Slowest, largest, cheapest)

*   **CPU Cache**: A small amount of very fast memory placed between the CPU and the main system memory. It stores frequently accessed data and instructions, allowing the CPU to avoid the slow trip to main memory. **L1 cache** is the smallest and fastest, typically embedded directly into each CPU core.

*   **Communication Buses**: These are the data highways that connect the various components (CPU to memory, CPU to I/O devices). The speed of these buses can be a major performance bottleneck.

*   **I/O (Input/Output) Devices**: Peripherals that allow the computer to interact with the outside world, such as keyboards, monitors, printers, and network cards.

##### 1.5 Processors vs. FPGAs
While both are silicon chips, CPUs and FPGAs have fundamentally different purposes.

*   **Processor (CPU)**: A CPU is a *fixed* device. Its internal logic circuits are permanently designed to execute a specific, unchangeable set of commands known as its **Instruction Set Architecture (ISA)** (e.g., x86, ARM). You cannot change what a CPU does; you can only provide it with different software instructions from its predefined set.

*   **FPGA (Field Programmable Gate Array)**: An FPGA is *not* a processor. It is a blank slate of reconfigurable logic circuits. A developer can program an FPGA to create a custom hardware design, which could be a custom processor, a graphics pipeline, or any other digital circuit. Its key features are:
    *   It can be **reprogrammed** multiple times.
    *   You design the hardware itself, supporting **only the instructions you need**, which can make it much more efficient for specific tasks.
    *   FPGAs are often used to **test and prototype** new processor designs before committing to the expensive manufacturing of a fixed CPU.

#### 2. Definitions

*   **Computer Architecture**: The design and organization of a computer system, focusing on the parts of the system visible to a programmer, primarily the Instruction Set Architecture (ISA).
*   **CPU (Central Processing Unit)**: The hardware component within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control, and input/output (I/O) operations specified by the instructions.
*   **ALU (Arithmetic Logic Unit)**: A digital circuit inside the CPU that performs arithmetic (add, subtract, etc.) and logic (AND, OR, NOT) operations on integer binary numbers.
*   **CU (Control Unit)**: The component of the CPU that directs the operation of the processor. It tells the computer's memory, arithmetic logic unit, and input and output devices how to respond to the instructions that have been sent to the processor.
*   **Register**: One of a small set of data holding places that are part of the CPU. A register can hold an instruction, a storage address, or any kind of data.
*   **Memory Hierarchy**: A structure that uses a hierarchy of memory and storage devices to optimize performance. The levels are tiered based on speed, cost, and capacity, from fast, small, expensive registers to slow, large, cheap storage.
*   **CPU Cache**: A smaller, faster memory, closer to a processor core, which stores copies of the data from frequently used main memory locations.
*   **Instruction Set Architecture (ISA)**: The part of the computer architecture related to programming, including the native data types, instructions, registers, addressing modes, and memory architecture. It is the abstract model of a computer.
*   **FPGA (Field Programmable Gate Array)**: An integrated circuit designed to be configured by a customer or a designer after manufacturing—hence the term "field-programmable."

#### 3. Mistakes

*   **Confusing FPGAs with CPUs**: Treating an FPGA as just another type of processor.
    **Why it's wrong**: A CPU has a fixed, unchangeable hardware design built to execute a predefined instruction set. An FPGA is a "blank canvas" of programmable logic gates that has no inherent function until a hardware design is loaded onto it. An FPGA can be programmed *to be* a CPU, but it isn't one by default.
*   **Ignoring the Memory Hierarchy**: Writing software with the assumption that all memory accesses (e.g., to RAM or an SSD) are equally fast.
    **Why it's wrong**: Accessing data in the L1 cache can be over 100 times faster than accessing it from main RAM. Efficient programs are designed to maximize **cache hits** (finding data in the cache) and minimize **cache misses** (having to fetch data from slower memory levels).
*   **Believing More Gigahertz (GHz) is Always Better**: Judging a processor's performance solely by its clock speed.
    **Why it's wrong**: Clock speed is only one piece of the puzzle. A processor with a lower clock speed but a more efficient ISA (e.g., executing more work per cycle), more cores, or a larger, faster cache can easily outperform a processor with a higher clock speed.
*   **Treating All CPU Architectures as the Same**: Assuming that a program compiled for one type of processor (like an Intel x86 chip) will run on another (like an Apple M-series ARM chip).
    **Why it's wrong**: Different architectures have different ISAs, meaning they understand completely different sets of machine-language instructions. Software must be compiled specifically for the target architecture.

#### 4. Examples

##### Example 1: Memory Hierarchy Access Time
**Question:** A program needs to process a large array of numbers. The programmer writes a loop that reads the first element, then the last element, then the second element, then the second-to-last, and so on, working from the outside in. Another programmer writes a simple loop that processes the elements in sequential order (1st, 2nd, 3rd, ...). Which program is likely to be faster, and why?

<details>
<summary>Click to see the solution</summary>

1.  **Analyze the access pattern:** The first program jumps back and forth across a large memory region. The second program accesses contiguous memory locations one after another.
2.  **Consider how caching works:** When the CPU requests data from a memory address, it doesn't just load that single piece of data into the cache. It loads a whole block of *adjacent* data (called a cache line), anticipating that the program will need nearby data soon.
3.  **Evaluate the first program:** The "outside-in" access pattern defeats the purpose of the cache. After accessing the first element, the CPU caches the next few elements. However, the program immediately jumps to the end of the array, causing a **cache miss**. This process repeats, leading to many cache misses and slow performance.
4.  **Evaluate the second program:** The sequential access pattern works perfectly with the cache. After accessing the first element, the next several elements are already loaded into the cache. The subsequent reads are therefore **cache hits**, which are extremely fast.

**Answer:** The **second program (sequential access)** will be significantly faster because its memory access pattern maximizes the use of the CPU cache and results in a high number of cache hits.
</details>

##### Example 2: CPU vs. FPGA Application
**Question:** You are tasked with designing a network router that must inspect every single data packet for a specific malicious pattern at extremely high speeds (billions of packets per second). Would a high-end, general-purpose CPU or an FPGA be a better choice for the core of this device? Explain your reasoning.

<details>
<summary>Click to see the solution</summary>

1.  **Define the problem:** The task is very specific, highly repetitive, and demands extreme performance and parallelism (inspecting many packets simultaneously).
2.  **Evaluate the CPU solution:** A CPU would have to run a program that loops through each packet's data. While a multi-core CPU can process several packets at once, its architecture is generalized and carries overhead for things not needed by this specific task (like running an OS). The sequential nature of executing instructions limits its ultimate throughput.
3.  **Evaluate the FPGA solution:** An FPGA can be programmed to create a massively parallel hardware pipeline. You can design a dedicated circuit where each stage performs one part of the inspection. As a packet flows through the circuit, it is being inspected. You can instantiate dozens or hundreds of these pipelines on a single FPGA, all running simultaneously at hardware speed.
4.  **Compare:** The FPGA can be tailored *exactly* to the task. It doesn't waste resources on general-purpose features. Its inherent parallelism is a perfect match for the problem of inspecting countless independent data packets.

**Answer:** The **FPGA** would be the better choice. It allows for the creation of a custom, massively parallel hardware architecture specifically designed for the packet inspection task, which will deliver far greater throughput than a general-purpose CPU running software.
</details>

##### Example 3: The Role of the Instruction Set
**Question:** A very simple CPU's instruction set only has three commands:
- `LOAD R1, mem_address`: Loads a value from a memory address into Register 1.
- `LOAD R2, mem_address`: Loads a value from a memory address into Register 2.
- `ADD R1, R2`: Adds the value in R2 to R1, storing the result in R1.
Assuming the number `5` is stored at memory address `100` and the number `10` is stored at memory address `104`, write the sequence of instructions to calculate `5 + 10`.

<details>
<summary>Click to see the solution</summary>

1.  **Goal:** Get the numbers 5 and 10 into the CPU's registers so the ALU can operate on them.
2.  **Step 1: Load the first number.** The `LOAD R1` instruction is needed to bring the value from memory address `100` into the first register.
    `LOAD R1, 100`
3.  **Step 2: Load the second number.** The `LOAD R2` instruction is needed to bring the value from memory address `104` into the second register.
    `LOAD R2, 104`
4.  **Step 3: Perform the addition.** Now that both operands are in registers, the `ADD` instruction can be executed. The result (15) will be stored in Register 1, overwriting the `5` that was there.
    `ADD R1, R2`

**Answer:** The sequence of instructions is:
**
```
LOAD R1, 100
LOAD R2, 104
ADD R1, R2
```
**
After these three instructions execute, Register 1 will hold the value `15`.
</details>