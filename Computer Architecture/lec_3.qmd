---
title: "3. Von Neumann and Harvard Architectures, Performance Metrics of Computers, Combinational Logic Circuits"
author: "Zakhar Podyakov"
date: "September 19, 2025"
format: html
engine: knitr
---

{{< video lec_3.mp4 >}}

[Quiz](https://notebooklm.google.com/notebook/89f0f798-2a08-41bf-8e09-536830fb1adc?artifactId=4a3fca60-7821-4db2-b2e5-1e5331288ea0) | [Flashcards](https://notebooklm.google.com/notebook/89f0f798-2a08-41bf-8e09-536830fb1adc?artifactId=8e033034-768a-41e7-9b11-8b066ceebcc2)

### **1. Summary**

##### **1.1 Computer Architectures**

###### **1.1.1 Harvard Architecture**
The **Harvard Architecture** is a computer architecture that utilizes physically separate storage and signal pathways for instructions and data. The CPU can read an instruction and perform a data memory access at the same time, even without a cache.

-   **Principle**: Instructions and data are stored in different physical memory units, each with dedicated buses.
-   **Memory Access**: The instruction memory is typically read-only, while the data memory is read-write.
-   **Pros**: It features lower performance overheads because instructions and data can be fetched concurrently, leading to faster execution.
-   **Cons**: The primary drawback is the increased hardware complexity and cost required to implement separate memory systems and buses.
-   **Usage**: It is commonly found in specialized systems like microcontrollers and Digital Signal Processors (DSPs), as well as in the internal cache structure of most modern CPUs.

<!-- DIAGRAM HERE -->

###### **1.1.2 Von Neumann Architecture**
Also known as the Princeton or Stored-Program architecture, the **Von Neumann Architecture** is the dominant design for general-purpose computers. It is based on the concept of storing both program instructions and the data they operate on in the same memory.

-   **Principle**: A single memory unit and a single bus are used for both instructions and data. Instructions are typically executed sequentially.
-   **Pros**: This design simplifies memory management and hardware implementation, making it more cost-effective and flexible.
-   **Cons**: It creates a performance limitation known as the *Von Neumann bottleneck*, where the shared bus becomes a chokepoint as the CPU cannot fetch instructions and data simultaneously.
-   **Usage**: This architecture is used in virtually all modern general-purpose computers, including desktops, laptops, and servers.

<!-- DIAGRAM HERE -->

##### **1.2 Performance Metrics**

###### **1.2.1 Program-Level Metrics**
These metrics evaluate the performance of a specific program's execution.

-   **Latency (or Delay/Response Time)**: This is the time elapsed between a program's invocation (the call) and the start of its actual execution. This delay is often caused by preparatory activities such as operating system overhead or memory allocation.
-   **Execution Time**: This measures the total time from the program's invocation until its completion. It includes both latency and the actual program execution time.

###### **1.2.2 Hardware-Level Metrics**
These metrics describe the capabilities of the hardware platform.

-   **Bandwidth**: The theoretical maximum number of jobs (or instructions) that can be executed concurrently. For example, a single-core processor has a bandwidth of 1, while a multi-core processor has a bandwidth equal to the number of cores.
-   **Throughput**: The *average* number of concurrently executing jobs over a period of time. It reflects the actual performance, which is often lower than the theoretical bandwidth.
-   **Utilization**: The percentage of time a specific hardware component is actively in use. For example, a CPU utilization of 57% means the CPU was busy for 57% of the measurement period.

###### **1.2.3 CPU vs. GPU Performance Philosophy**
CPUs and GPUs are designed with different performance goals.

-   **CPU (Latency Optimized)**: A CPU is like a sports car. It is designed to execute a small number of tasks (or threads) as quickly as possible, minimizing latency for each one.
-   **GPU (Throughput Optimized)**: A GPU is like a bus. It is designed to handle a massive number of tasks simultaneously, maximizing overall throughput, even if the latency for any single task is higher than a CPU's.

##### **1.3 Processor and System Performance Characteristics**

###### **1.3.1 CPU Clock**
The speed of a processor is governed by its internal clock.

-   **Clock Cycle**: A CPU operates on a periodic clock signal. The time it takes to complete one on-off cycle is the **clock cycle time** or **period**, denoted as $T_c$ (measured in nanoseconds).
-   **Clock Frequency (or Rate)**: This is the inverse of the clock cycle time ($F_c = 1 / T_c$) and represents the number of cycles the CPU can execute per second, measured in Hertz (Hz) or Gigahertz (GHz). The duration of a clock cycle is determined by the time required for the slowest instruction to complete.

###### **1.3.2 Average Performance of Computer Components**
The following are typical performance characteristics for modern desktop and laptop components.

-   **CPU (Central Processing Unit)**:
    -   Cores: 4-6
    -   Speed: 2.0-3.5 GHz (up to 5.0 GHz in Turbo mode)
    -   Capacity: 32 or 64 bits per register
-   **Cache Memory**:
    -   **L1 Cache**: Operates at CPU speed (~100x faster than RAM) with a capacity of 32-128 KB.
    -   **L2 Cache**: About 25x faster than RAM, with a capacity of 256-512 KB.
    -   **L3 Cache**: About 2x faster than RAM, with a capacity of 32-64 MB.
-   **System Memory (RAM)**:
    -   Speed: 18-36 GB/sec
    -   Capacity: 8-32 GB
-   **Storage Devices**:
    -   **SSD (Solid State Drive)**: Speed of 0.5-2 GB/sec, capacity of 250-500 GB.
    -   **HDD (Hard Disk Drive)**: Speed of 0.1-0.15 GB/sec, capacity of 100-150 GB.
-   **GPU (Graphics Processing Unit)**:
    -   Cores: 700-2000 (structurally simpler than CPU cores)
    -   Speed: 1.5-2.0 GHz
    -   Architecture: Optimized for parallel execution (SIMD), making it ~100x faster than a CPU for highly parallel tasks.
    -   GPU Memory Speed: 300-500 GB/sec
    -   GPU Memory Capacity: 8-16 GB

##### **1.4 Amdahl's Law**
**Amdahl's Law** provides a formula to find the maximum expected improvement to an overall system when only part of the system is improved. It is often used to predict the theoretical maximum speedup of a program when using multiple processors.

-   **Concept**: Any program consists of two parts: a *serial portion* that must be executed sequentially and a *parallel portion* that can be divided among multiple cores.
-   **Implication**: The speedup gained from adding more cores is ultimately limited by the serial portion of the program. Even with an infinite number of processors, the total execution time can never be less than the time required to run the serial part.
-   **Formula**: The speedup of a program is given by:
    $$ \text{speedup} = \frac{1}{x + \frac{1-x}{m}} $$
    where $x$ is the proportion of the program that is serial, and $m$ is the number of processor cores.

##### **1.5 Combinational Logic Circuits**

###### **1.5.1 Foundational Terminology**
-   **Boolean Function**: A function that takes one or more binary inputs (0 or 1) and produces a single binary output. It is the mathematical foundation of digital circuits, introduced by George Boole.
-   **Truth Table**: A table that exhaustively lists all possible input combinations to a Boolean function and shows the corresponding output for each combination.
-   **Logic Gate**: An electronic device, typically built from transistors, that implements a basic Boolean function. Examples include AND, OR, and NOT gates.
-   **Logic Circuit**: A collection of interconnected logic gates designed to perform a more complex Boolean function. The output of one gate can become the input for another.

###### **1.5.2 Basic Logic Gates**
-   **AND**: Outputs 1 only if *all* of its inputs are 1.
-   **OR**: Outputs 1 if *at least one* of its inputs is 1.
-   **NOT**: Outputs the inverse of its single input (1 becomes 0, 0 becomes 1).
-   **NAND (Not-AND)**: Outputs the inverse of an AND gate. It is 0 only when all inputs are 1.
-   **NOR (Not-OR)**: Outputs the inverse of an OR gate. It is 1 only when all inputs are 0.
-   **XOR (Exclusive-OR)**: Outputs 1 only if the inputs are different. Unlike an OR gate, it outputs 0 if both inputs are 1.
-   **XNOR (Exclusive-NOR)**: Outputs the inverse of an XOR gate. It outputs 1 only if the inputs are the same.

<!-- DIAGRAM HERE -->

###### **1.5.3 Universal Logic Gates**
A **universal gate** is a logic gate that can be used to implement any other logic gate or Boolean function without needing any other type of gate.

-   **NAND and NOR Gates**: Both NAND and NOR gates are universal. For instance, a NOT gate can be made from a NAND gate by connecting its inputs together. An AND gate can be made from two NAND gates, and an OR gate from three.
-   **Significance**: This property is crucial in digital circuit design because it means that any complex logic circuit can be constructed using only one type of gate, which simplifies the manufacturing process.

### **2. Definitions**

*   **Boolean Function**: A mathematical function that takes binary inputs (values of 0 or 1) and produces a single binary output.
*   **Truth Table**: A tabular representation of a Boolean function that lists all possible input combinations and their corresponding outputs.
*   **Logic Gate**: A physical electronic device that performs a basic logical operation, implementing a Boolean function.
*   **Logic Circuit**: A structure composed of interconnected logic gates designed to implement a more complex Boolean function.
*   **Universal Gate**: A logic gate (such as NAND or NOR) from which any other logic gate can be constructed.
*   **Harvard Architecture**: A computer architecture with separate memory and buses for instructions and data, allowing for simultaneous access.
*   **Von Neumann Architecture**: A computer architecture that uses a single memory storage system for both instructions and data.
*   **Von Neumann Bottleneck**: A performance limitation in Von Neumann architectures caused by the shared bus between the CPU and memory, which prevents simultaneous fetching of instructions and data.
*   **Latency**: The time delay between the initiation of a process and the start of its actual execution.
*   **Execution Time**: The total time required for a program to run from invocation to completion.
*   **Bandwidth**: The theoretical maximum rate at which data or instructions can be processed by a system.
*   **Throughput**: The actual average rate at which data or instructions are processed over a given period.
*   **Amdahl's Law**: A formula used to calculate the maximum potential speedup of a system when only a part of it is improved.

### **3. Formulas**

*   **Clock Frequency**: $F_c = \frac{1}{T_c}$, where $T_c$ is the clock cycle time.
*   **Amdahl's Law Speedup**: $\text{speedup} = \frac{1}{x + \frac{1-x}{m}}$, where $x$ is the serial portion and $m$ is the number of cores.
*   **Boolean AND**: $F(A, B) = A \cdot B$
*   **Boolean OR**: $F(A, B) = A + B$
*   **Boolean NOT**: $F(A) = A'$

### **4. Mistakes**

*   **Confusing Harvard and Von Neumann architectures:** A common error is mixing up their core principles. **Why it's wrong:** Harvard uses separate memories for instructions and data, whereas Von Neumann uses a single, shared memory. This fundamental difference dictates their respective strengths and weaknesses.
*   **Misinterpreting Amdahl's Law:** Believing that doubling the number of cores will double a program's speed. **Why it's wrong:** Amdahl's Law shows that the serial portion of a program creates a hard limit on the maximum achievable speedup, which is often far less than linear.
*   **Using Throughput and Bandwidth interchangeably:** Treating these terms as synonyms. **Why it's wrong:** Bandwidth is the theoretical peak performance (e.g., the speed limit on a highway), while throughput is the actual, measured performance (e.g., the average speed of traffic), which is affected by real-world factors.
*   **Treating an XOR gate as an OR gate:** Forgetting the "exclusive" property of XOR. **Why it's wrong:** An OR gate outputs true (1) if one or both inputs are true. An XOR gate outputs true (1) only if the inputs are different; it outputs false (0) if both inputs are true.
*   **Underestimating the Von Neumann Bottleneck:** Ignoring the performance impact of the shared bus for data and instructions. **Why it's wrong:** This bottleneck is a critical constraint in modern computing, as CPUs often have to wait for the bus to be free, and it is a primary motivation for developing complex memory hierarchies like caches.

### **5. Examples**

##### **5.1. Truth Table for a Boolean Function**
**Question:** Create a truth table for the Boolean function $F(A, B, C) = (A \cdot B) + C$.
<details>
<summary>Click to see the solution</summary>
We need to evaluate the function for all 8 possible combinations of inputs A, B, and C. We first compute the AND term ($A \cdot B$) and then OR it with C.

| A | B | C | A · B | (A · B) + C |
|:-:|:-:|:-:|:-----:|:-----------:|
| 0 | 0 | 0 |   0   |       0       |
| 0 | 0 | 1 |   0   |       1       |
| 0 | 1 | 0 |   0   |       0       |
| 0 | 1 | 1 |   0   |       1       |
| 1 | 0 | 0 |   0   |       0       |
| 1 | 0 | 1 |   0   |       1       |
| 1 | 1 | 0 |   1   |       1       |
| 1 | 1 | 1 |   1   |       1       |

</details>

##### **5.2. Implementing an OR Gate with NAND Gates**
**Question:** Show how to construct an OR gate using only two-input NAND gates.
<details>
<summary>Click to see the solution</summary>
The Boolean expression for an OR gate is $A + B$. Using De Morgan's laws, we know that $A + B = \overline{\overline{A} \cdot \overline{B}}$. This expression can be implemented with NAND gates.

1.  **Create NOT gates:** To get $\overline{A}$ (NOT A) and $\overline{B}$ (NOT B), we use two NAND gates. For the first gate, connect both inputs to A. For the second, connect both inputs to B.
2.  **Combine the results:** Feed the outputs of these two NOT gates ($\overline{A}$ and $\overline{B}$) into a third NAND gate. The output of this final gate will be $\overline{\overline{A} \cdot \overline{B}}$, which is equivalent to $A + B$.

<!-- DIAGRAM HERE -->

**Answer:** It requires three NAND gates to implement a two-input OR gate.

</details>

##### **5.3. Amdahl's Law Calculation**
**Question:** A program spends 80% of its execution time in a parallelizable section. What is the maximum possible speedup if you run this program on a machine with 4 cores?
<details>
<summary>Click to see the solution</summary>
1.  **Identify variables:**
    -   The parallelizable portion is 80%, so $1 - x = 0.8$.
    -   The serial portion is the remainder, so $x = 0.2$.
    -   The number of cores is $m = 4$.
2.  **Apply Amdahl's Law formula:**
    $$ \text{speedup} = \frac{1}{x + \frac{1-x}{m}} $$
    $$ \text{speedup} = \frac{1}{0.2 + \frac{0.8}{4}} $$
3.  **Calculate the result:**
    $$ \text{speedup} = \frac{1}{0.2 + 0.2} = \frac{1}{0.4} = 2.5 $$

**Answer:** The maximum speedup with 4 cores is **2.5x**.

</details>

##### **5.4. Clock Cycle Time Calculation**
**Question:** A CPU has a clock rate of 3.2 GHz. What is its clock cycle time in nanoseconds?
<details>
<summary>Click to see the solution</summary>
1.  **Convert GHz to Hz:**
    -   $3.2 \text{ GHz} = 3.2 \times 10^9 \text{ Hz}$
2.  **Use the formula $T_c = 1 / F_c$:**
    -   $T_c = \frac{1}{3.2 \times 10^9 \text{ Hz}} \approx 0.3125 \times 10^{-9} \text{ seconds}$
3.  **Convert seconds to nanoseconds:**
    -   Since $1 \text{ ns} = 10^{-9} \text{ s}$, the clock cycle time is $0.3125$ ns.

**Answer:** The clock cycle time is **0.3125 ns**.

</details>

##### **5.5. Architecture Identification**
**Question:** A microcontroller for an industrial robot has two separate memory blocks: one for the control program (firmware) and one for storing sensor readings. Which computer architecture does this design follow?
<details>
<summary>Click to see the solution</summary>
The design uses separate memory for the program (instructions) and for sensor readings (data). This is the defining characteristic of the Harvard architecture, which allows the controller to fetch the next instruction while simultaneously accessing sensor data, improving real-time performance.

**Answer:** The **Harvard Architecture**.

</details>

##### **5.6. XOR Gate Logic**
**Question:** A logic circuit uses an XOR gate to compare two 1-bit inputs, A and B. When will the output be 1?
<details>
<summary>Click to see the solution</summary>
An XOR (Exclusive-OR) gate outputs 1 only when its inputs are different. We can verify this with its truth table:

| A | B | Output |
|:-:|:-:|:------:|
| 0 | 0 |    0     |
| 0 | 1 |    1     |
| 1 | 0 |    1     |
| 1 | 1 |    0     |

**Answer:** The output will be 1 when `A=0` and `B=1`, or when `A=1` and `B=0`.

</details>

##### **5.7. Performance Metric Analysis**
**Question:** System A can process a video file in 10 minutes. System B can process the same file in 15 minutes, but it can process two such files simultaneously, finishing both in 30 minutes. Which system has better latency, and which has better throughput?
<details>
<summary>Click to see the solution</summary>
1.  **Latency Analysis**: Latency refers to the time to complete a single task.
    -   System A finishes one file in 10 minutes.
    -   System B finishes one file in 15 minutes.
    -   Therefore, **System A has better (lower) latency**.
2.  **Throughput Analysis**: Throughput refers to the total work done over time.
    -   In 30 minutes, System A processes 3 files ($30 / 10 = 3$).
    -   In 30 minutes, System B processes 2 files.
    -   Therefore, **System A has better (higher) throughput**.

**Answer:** System A is superior in both latency and throughput for this specific workload.

</details>