---
title: "2. Hierarchy of Memories, Moore's Law, Parallelism, Pipelining"
author: "Zakhar Podyakov"
date: "September 17, 2025"
format: html
engine: knitr
---

#### **1. Summary**

##### **1.1 The Hierarchy of Memories**
In computer architecture, the **memory hierarchy** is a fundamental concept that organizes a computer's storage into a pyramid-like structure. This organization is necessary to balance three competing factors: *speed*, *capacity*, and *cost*. Processors are extremely fast, but high-speed memory is expensive and thus has a small capacity. Conversely, large-capacity storage is affordable but much slower. The memory hierarchy solves this problem by creating layers of memory, where each level is smaller, faster, and more expensive per byte than the level below it.

The typical levels of the memory hierarchy, from fastest to slowest, are:

1.  **CPU Registers**: These are the fastest and smallest memory units, located directly inside the CPU. They hold the data that the CPU is currently processing. Access is virtually instantaneous.
2.  **Cache Memory**: This is a small, very fast memory that sits between the CPU and the main system memory. It stores frequently accessed data and instructions, allowing the CPU to retrieve them much faster than from the main memory. Caches are typically divided into levels (L1, L2, L3), with L1 being the smallest and fastest.
3.  **System Memory (RAM - Random Access Memory)**: This is the computer's main working memory, where the operating system, applications, and data in current use are kept so that they can be quickly reached by the computer's processor. It is significantly larger than cache but also slower.
4.  **Storage Devices (Secondary Storage)**: This includes devices like Solid-State Drives (SSDs) and Hard Disk Drives (HDDs). They provide long-term, high-capacity storage for data and programs. This is the slowest but largest level of the hierarchy.

<!-- DIAGRAM HERE -->

An important distinction within the hierarchy is between **volatile** and **non-volatile** memory.

*   *Volatile Memory* (Registers, Cache, RAM) requires power to maintain the stored information. It loses all its data when the power is turned off.
*   *Non-Volatile Memory* (SSDs, HDDs, Flash Memory) retains its stored information even when not powered.

##### **1.2 Design Simplification via Abstraction**
**Abstraction** is a core principle in computer architecture used to manage complexity. It involves hiding the complex details of a system while exposing only the essential features. This allows designers and programmers to work with a simplified model of a component without needing to understand its intricate internal workings.

For example, a CPU can be viewed at several levels of abstraction:

*   **Highest Level:** A programmer sees the CPU as a single block that executes instructions.
*   **Intermediate Level:** An architect sees the CPU as a collection of major components, such as the Control Unit (CU), Arithmetic Logic Unit (ALU), and registers.
*   **Lowest Level:** An engineer sees the CPU as a detailed diagram of logic gates and transistors that implement its functions.

By using abstraction, a complex system like a computer can be designed, built, and programmed in manageable layers.

##### **1.3 Moore's Law and Its Stagnation**
**Moore's Law** is an observation made by Intel co-founder Gordon Moore in 1965. It states that the number of transistors on an integrated circuit (IC) doubles approximately every two years. For decades, this trend also meant that CPU execution speed doubled every 18-24 months, leading to exponential growth in computing power.

However, since around 2008, the growth in single-thread performance and CPU clock speed has significantly slowed down, leading to what is often called the stagnation of Moore's Law. This is not because transistor density has stopped increasing, but because of two fundamental physical limits:

1.  **Heat Dissipation:** As transistors become smaller and more densely packed, the heat they generate becomes a major problem. Increasing clock speed further leads to higher power consumption and excessive heat, which can damage the chip. This is known as the end of *Dennard scaling*.
2.  **Speed of Light Limitation:** Signals within a CPU chip travel at nearly the speed of light. As chips get faster, the time it takes for a signal to travel across the chip becomes a significant limiting factor.

Because of these limitations, the industry has shifted its focus from making single processors faster to adding more processors (or **cores**) to a single chip. This has led to the rise of *multicore* and *multiprocessor* systems.

##### **1.4 Performance via Parallelism**
**Parallelism**, or parallel processing, involves using multiple processing units to execute multiple tasks or parts of a single task simultaneously. This contrasts with a single-processor system, which executes instructions sequentially (one after another).

The goal of multiprocessing is to speed up computation by dividing work among multiple cores. This is highly effective for tasks that can be broken down into independent sub-tasks. However, a major challenge is *instruction dependency*, where one instruction needs the result of a previous one before it can execute, forcing a sequential execution and limiting the benefits of parallelism.

##### **1.5 Performance via Pipelining**
**Pipelining** is another technique to improve performance, but it works differently from parallelism. Instead of using multiple processors, pipelining uses a single processor and breaks down the execution of an instruction into several stages. It then overlaps these stages for different instructions, much like an assembly line.

A classic five-stage pipeline includes:

1.  **Instruction Fetch (IF):** Fetch the instruction from memory.
2.  **Instruction Decode (ID):** Decode the instruction to see what it does.
3.  **Execute (EX):** Perform the calculation.
4.  **Memory Access (MEM):** Read from or write to memory.
5.  **Write Back (WB):** Write the result back to a register.

While one instruction is being executed (EX stage), the next one is being decoded (ID), and the one after that is being fetched (IF). This allows the CPU to work on multiple instructions at once, increasing throughput. *Pipelining improves performance by increasing instruction throughput, while parallelism improves performance by running multiple instructions simultaneously on different hardware.*

##### **1.6 Performance via Speculation (Prediction)**
**Speculative execution** is an optimization technique where a processor makes an educated guess about the future execution path of a program and begins executing instructions from that predicted path before it's certain the path will be taken. This is most commonly used for *branch prediction*.

When the CPU encounters a conditional branch (e.g., an `if` statement), instead of waiting to see which branch is taken, the *branch predictor* guesses the outcome. The CPU then speculatively executes instructions along the predicted path.
*   If the prediction was **correct**, the results are kept, and time was saved by avoiding a pipeline stall.
*   If the prediction was **incorrect**, the speculative results are discarded, and the CPU starts executing from the correct path. This incurs a performance penalty, but since modern predictors are highly accurate (>95%), the overall performance gain is significant.

##### **1.7 Other Fundamental Ideas**
*   **Dependability via Redundancy**: This principle involves adding spare components (e.g., extra CPUs, memory, or power supplies) to a system to increase its reliability. If a primary component fails, a redundant one can take over, ensuring the system continues to operate. This is critical in applications like spacecraft or servers.
*   **Make the Common Case Fast**: This design philosophy prioritizes optimizing the performance of the most frequent operations. By focusing resources on making common tasks as fast as possible, the overall system performance is improved, even if less common tasks are not as optimized.
*   **Finite State Machines (FSM)**: An FSM is a mathematical model of computation used to design both hardware and software. It consists of a finite number of *states* and the *transitions* between them in response to inputs. FSMs are a powerful tool for modeling the behavior of systems like network protocols, compilers, and hardware components like processor caches.

#### **2. Definitions**

*   **Computer Architecture**: The design and fundamental operational structure of a computer system. It defines the system's parts and their interrelationships, especially the instruction set architecture (ISA), microarchitecture, and system design.
*   **CPU (Central Processing Unit)**: The primary component of a computer that executes instructions. It contains the Control Unit and the Arithmetic Logic Unit.
*   **ALU (Arithmetic Logic Unit)**: The part of the CPU that performs arithmetic (addition, subtraction) and logic (AND, OR, NOT) operations.
*   **Control Unit (CU)**: The part of the CPU that directs the operation of the processor. It tells the other parts of the computer system how to carry out a program's instructions.
*   **Register**: A small, extremely fast memory location directly inside the CPU used to hold a single piece of data during processing.
*   **Cache Memory**: A small amount of very fast, expensive memory used to store frequently accessed data from the main memory, reducing the average time to access data.
*   **System Memory (RAM)**: Volatile hardware in a computing device where the operating system, application programs, and data in current use are kept so they can be quickly reached by the device's processor.
*   **Volatile Memory**: Memory that requires constant power to maintain stored information; its contents are lost when power is turned off.
*   **Non-Volatile Memory**: Memory that can retain stored information even after power is removed.
*   **Abstraction**: The technique of hiding complex implementation details and showing only the necessary features of an object or system.
*   **Moore's Law**: The observation that the number of transistors in an integrated circuit doubles about every two years.
*   **Parallelism**: The simultaneous execution of multiple instructions or tasks on multiple processing units (cores) to achieve faster computation.
*   **Pipelining**: A technique where a single processor overlaps the execution of multiple instructions by breaking each into stages and processing them in an assembly-line fashion.
*   **Speculative Execution**: An optimization technique where a processor performs a task before it is known to be needed, such as executing instructions after a predicted branch.
*   **Redundancy**: The inclusion of extra components that are not strictly necessary to functioning, in case of failure in other components.
*   **Finite State Machine (FSM)**: A computational model consisting of a finite number of states and transitions between them, used to model the behavior of systems.

#### **3. Mistakes**

*   **Confusing Parallelism and Pipelining:** Thinking these are the same concept. **Why it's wrong:** Parallelism involves using multiple, independent processing units (like multiple cores) to execute different tasks simultaneously. Pipelining involves a single processing unit breaking instructions into stages and overlapping these stages to increase throughput. Pipelining is a form of instruction-level parallelism, but it doesn't use multiple CPUs.
*   **Believing Moore's Law Guarantees Faster Clock Speeds:** Assuming that the doubling of transistors automatically translates to a doubling of CPU clock frequency. **Why it's wrong:** While historically correlated, clock speeds have stagnated since the mid-2000s due to power and heat dissipation limits. Moore's Law now primarily results in more cores on a chip rather than faster individual cores.
*   **Assuming All Memory is Equal:** Ignoring the vast performance differences between registers, cache, RAM, and SSDs. **Why it's wrong:** The memory hierarchy exists because there is a trade-off between speed and cost. An algorithm that frequently accesses data from RAM or disk will be orders of magnitude slower than one that keeps its working data in the cache.
*   **Ignoring the Cost of a Branch Misprediction:** Believing that speculative execution is a "free" performance boost. **Why it's wrong:** When a branch is mispredicted, the entire pipeline must be flushed of the incorrect, speculatively executed instructions, and the correct instructions must be fetched. This incurs a significant performance penalty. The technique is only effective because modern predictors are correct most of the time.
*   **Thinking More Cores Always Means a Faster Program:** Assuming that doubling the number of CPU cores will cut a program's execution time in half. **Why it's wrong:** This is only true for perfectly parallelizable tasks. Most programs have sequential parts and dependencies between threads, which limit the speedup gained from adding more cores, a concept described by Amdahl's Law.
*   **Treating Abstraction as Just "Dumbing Down":** Viewing abstraction as merely a simplification that loses important information. **Why it's wrong:** Abstraction is a critical tool for managing complexity. It allows engineers and programmers to work effectively at different levels of a system without being overwhelmed by irrelevant detail, which is essential for building and maintaining complex modern computers.

#### **4. Examples**

##### **4.1. Memory Hierarchy Ordering**
**Question:** A program needs to access a piece of data. Arrange the following memory types in order from the location that would provide the fastest access to the one that would provide the slowest access: L2 Cache, SSD, System RAM, CPU Register.

<details>
<summary>Click to see the solution</summary>

1.  **Identify the fastest level:** The memory closest to the CPU's processing units is the fastest. CPU registers are physically part of the CPU core.
2.  **Consider the cache levels:** Cache is the next fastest level, acting as a buffer for RAM.
3.  **Place the main memory:** System RAM is the primary working memory but is slower than the cache.
4.  **Place the storage device:** Secondary storage like an SSD is the slowest level in this list, as it is accessed via I/O controllers and not directly by the CPU in the same way as RAM.

**Answer:** The correct order from fastest to slowest is: **CPU Register, L2 Cache, System RAM, SSD.**
</details>

##### **4.2. Pipelining Throughput**
**Question:** A non-pipelined processor takes 5 clock cycles to execute one instruction. A 5-stage pipelined processor has a clock cycle time of 1 ns and can complete one stage per cycle. Ignoring any pipeline stalls, how long would it take the pipelined processor to execute 10 instructions?

<details>
<summary>Click to see the solution</summary>

1.  **Calculate the time to fill the pipeline:** The first instruction will take the full number of stages to complete because the pipeline starts empty. Time for the first instruction = 5 stages * 1 ns/stage = 5 ns.
2.  **Calculate the time for subsequent instructions:** Once the pipeline is full, a new instruction completes every clock cycle. So, the remaining (10 - 1) = 9 instructions will each take 1 ns to emerge from the pipeline.
3.  **Sum the times:** Total time = (Time for first instruction) + (Time for the remaining 9 instructions) = 5 ns + (9 * 1 ns) = 14 ns.

**Answer:** It would take **14 ns** to execute 10 instructions.
</details>

##### **4.3. Identifying Parallelism vs. Pipelining**
**Question:** A server is running a web application and a database simultaneously. The server's CPU has 8 cores, and the operating system assigns the web application to run on 4 cores and the database to run on the other 4 cores. Is this an example of performance via parallelism or pipelining?

<details>
<summary>Click to see the solution</summary>

1.  **Analyze the resource allocation:** The system is using multiple, distinct processing units (8 cores).
2.  **Analyze the task distribution:** Two separate, large-scale tasks (web application and database) are being executed at the same time on different sets of cores.
3.  **Compare with definitions:** This matches the definition of parallelism, which uses multiple processors to handle different tasks concurrently. It is not pipelining, which involves overlapping stages of instructions on a single processor.

**Answer:** This is an example of **performance via parallelism**.
</details>

##### **4.4. Applying Moore's Law**
**Question:** In 2023, you buy a laptop with a CPU that has 8 cores. Based on the modern interpretation of Moore's Law (where performance gains come from more cores rather than faster clock speeds), what would be a reasonable expectation for a similar-class laptop CPU you might buy in 2027 (4 years later)?

<details>
<summary>Click to see the solution</summary>

1.  **Recall Moore's Law:** The number of transistors doubles roughly every 2 years. In the multicore era, this often translates to a doubling of cores.
2.  **Calculate the number of doubling periods:** The time frame is 4 years, which is two 2-year periods.
3.  **Apply the doubling:**
    *   After the first 2 years (2025), the core count would be expected to double from 8 to 16.
    *   After the second 2 years (2027), the core count would be expected to double again from 16 to 32.

**Answer:** A reasonable expectation would be a CPU with **32 cores**.
</details>

##### **4.5. Levels of Abstraction**
**Question:** Describe a modern car at three different levels of abstraction, from highest (simplest) to lowest (most detailed).

<details>
<summary>Click to see the solution</summary>

1.  **Highest Level (The Driver's View):** At this level, the car is a simple machine with a few key interfaces: a steering wheel to change direction, pedals for acceleration and braking, and a gear selector. The complex mechanics of the engine, transmission, and electronics are hidden. The car is an object that transports you from one place to another.
2.  **Intermediate Level (The Mechanic's View):** A mechanic sees the car as a system of interconnected components: the engine, the transmission, the braking system, the exhaust system, the electrical system, etc. They understand how these major parts work together and can diagnose problems within a specific system, but they might not know the internal details of the engine control unit (ECU).
3.  **Lowest Level (The Engineer's View):** An automotive engineer sees the car at a highly detailed level. They are concerned with the physics of combustion inside an engine cylinder, the material science of the brake pads, the specific algorithms running on the ECU, and the design of individual transistors on a microchip. This is the most complex and detailed view, where the fundamental principles of operation are designed and analyzed.

**Answer:** The three levels are **Driver (highest abstraction)**, **Mechanic (intermediate abstraction)**, and **Engineer (lowest abstraction)**.
</details>

##### **4.6. Speculative Execution Scenario**
**Question:** Consider the following piece of code. Explain how a CPU with speculative execution and branch prediction would handle the `if` statement to improve performance.
`if (user_input > 100) { perform_complex_calculation(); } else { perform_simple_task(); }`

<details>
<summary>Click to see the solution</summary>

1.  **Encounter the Branch:** When the CPU's pipeline reaches the `if` statement, it encounters a conditional branch. It needs to know the result of `user_input > 100` to decide which block of code to execute next.
2.  **Predict the Outcome:** Instead of waiting for the comparison to complete, the branch predictor makes a guess. Based on past behavior of this branch, it might predict that the condition is usually `false`.
3.  **Speculatively Execute:** The CPU will speculatively start fetching and executing the instructions for `perform_simple_task()` from the `else` block. It will do this *before* it officially knows the value of `user_input`.
4.  **Verify the Prediction:** In parallel, the CPU completes the `user_input > 100` comparison.
    *   **If the prediction was correct (`false`):** The results of `perform_simple_task()` are kept, and the pipeline continues without any delay. Performance was gained.
    *   **If the prediction was incorrect (`true`):** The CPU discards all the work it did on `perform_simple_task()`, flushes the pipeline, and begins fetching and executing instructions from the correct path: `perform_complex_calculation()`. Performance was lost compared to a perfect guess, but the system still functions correctly.

**Answer:** The CPU predicts the outcome of the `if` condition, executes the predicted code path (e.g., the `else` block) immediately, and only discards the work if the prediction was wrong.
</details>

##### **4.7. Finite State Machine Design**
**Question:** Design a simple Finite State Machine (FSM) for an automatic door. The door has a sensor that detects a person. It should have three states: `CLOSED`, `OPENING`, and `OPEN`. Describe the states and the transitions between them.

<details>
<summary>Click to see the solution</summary>

1.  **Define the States:**
    *   **`CLOSED`**: The initial state. The door is fully closed and is waiting for a trigger.
    *   **`OPENING`**: The door is in the process of opening. This is a transient state.
    *   **`OPEN`**: The door is fully open. It will wait here for a period before attempting to close.

2.  **Define the Transitions:**
    *   **`CLOSED` to `OPENING`**: This transition occurs when the input `person_detected` is `true`.
    *   **`OPENING` to `OPEN`**: This transition occurs automatically once the door has finished its opening motion (e.g., an `opening_complete` signal becomes `true`).
    *   **`OPEN` to `CLOSED`**: This transition occurs after a timer expires and the input `person_detected` is `false`. The door starts closing. (For simplicity, we can assume it transitions directly to `CLOSED`, though a `CLOSING` state could also be added).
    *   **`OPEN` to `OPEN`**: If the timer expires but a person is still detected (`person_detected` is `true`), the FSM should remain in the `OPEN` state and reset its timer to avoid closing on someone.

<!-- DIAGRAM HERE -->

**Answer:** The FSM has three states: **`CLOSED`**, **`OPENING`**, and **`OPEN`**. Transitions are triggered by a person being detected by the sensor and internal timers for the opening/closing process.
</details>