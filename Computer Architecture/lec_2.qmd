---
title: "2. Fundamental Ideas of Computer Architecture"
author: "Zakhar Podyakov"
date: "September 16, 2025"
format: html
---

#### 1. Summary

##### 1.1 Hierarchy of Memories
Computer systems use a variety of memory types, organized into a **hierarchy** based on their performance characteristics. This structure is a trade-off between *speed, capacity, and cost*. The levels are arranged like a pyramid: the fastest, smallest, and most expensive memory is at the top, closest to the CPU, while the slowest, largest, and cheapest is at the bottom.

1.  **CPU Registers:** These are the fastest memory units, located directly inside the **CPU** (Central Processing Unit). They hold the data that the CPU is actively manipulating at any given moment. Their capacity is extremely small (measured in bytes).
2.  **Cache Memory (L1, L2, L3):** A small, fast layer of memory that sits between the CPU and the main system memory (RAM). It stores copies of frequently used data from RAM. When the CPU needs data, it checks the cache first. If the data is there (a *cache hit*), it's retrieved much faster than from RAM. If not (a *cache miss*), the data is fetched from RAM and also stored in the cache for future use.
3.  **System Memory (RAM):** **Random-Access Memory** is the main working memory of the computer. It is much larger than the cache but also significantly slower. It holds the operating system, running applications, and the data they are using. RAM is *volatile*, meaning its contents are lost when the power is turned off.
4.  **Storage Devices (SSD, HDD):** These are used for long-term data storage. They are the largest and slowest level in the hierarchy and are *non-volatile*, meaning they retain data without power.

As data moves from the bottom of the hierarchy to the top, the *access speed increases* dramatically, but the *storage capacity decreases*.

##### 1.2 Design Simplification via Abstraction
**Abstraction** is a core principle used to manage complexity in computer architecture. It involves hiding the complex details of a system and exposing only the essential features. This allows designers and programmers to work with a system at a high level without needing to understand every detail of its low-level implementation.

For example, a programmer views the CPU as a simple black box that executes instructions. This is the highest level of abstraction. A computer architect, however, looks inside this box and sees components like the **Control Unit (CU)**, the **Arithmetic Logic Unit (ALU)**, and **registers**. A hardware engineer might go even deeper, designing the individual logic gates and circuits that make up the ALU. Each layer only needs to understand its own level and the interface to the levels directly above and below it, simplifying the overall design process. As you move from a high-level view to a low-level one, the *abstraction level decreases*, and the level of detail increases.

##### 1.3 Moore's Law for CPU Performance
**Moore's Law** is an observation made by Gordon Moore in 1965, stating that the number of transistors on an integrated circuit (CPU chip) *doubles approximately every two years*. For decades, this rapid increase in transistor density directly correlated with an exponential increase in CPU performance and execution speed.

However, since around 2008, the rate of increase for single-thread performance and CPU clock speed has stagnated. This is primarily due to two physical limitations:
1.  **Heat Dissipation:** Faster clock speeds lead to higher power consumption, which generates more heat. At a certain point, it becomes impossible to cool the chip effectively, causing it to overheat and fail.
2.  **The Speed of Light:** As transistors become smaller and closer, the time it takes for electrical signals to travel between them becomes a limiting factor.

Because of this stagnation, the industry shifted its focus from making single cores faster to adding more cores to a single chip. This means that performance gains today are primarily achieved through parallelism, not by increasing the speed of a single CPU core.

##### 1.4 Performance via Parallelism
**Parallelism** is the concept of executing multiple tasks or instructions *simultaneously* to improve performance. In contrast to a uniprocessor system that executes one instruction at a time, a **multiprocessor** or **multicore** system contains multiple processing units that can work on different tasks concurrently.

This approach is the primary driver of performance improvement in modern computing. For example, a quad-core processor can theoretically work on four different instructions at the exact same time. However, this introduces new challenges. The main problem is *data dependency*, where one instruction depends on the result of a previous one. Such instructions cannot be executed in parallel and must wait for the preceding one to complete, which requires complex scheduling and synchronization logic.

##### 1.5 Performance via Pipelining
**Pipelining** is a technique that increases the *throughput* of a single CPU, which is the number of instructions it can complete per unit of time. It doesn't execute multiple instructions simultaneously in the same way parallelism does; instead, it overlaps the execution steps of different instructions.

The execution of a single instruction is broken down into a series of independent stages, such as:

1.  **IF (Instruction Fetch):** Fetch the instruction from memory.
2.  **ID (Instruction Decode):** Decode the instruction to see what it does.
3.  **EX (Execute):** Perform the calculation in the ALU.
4.  **MEM (Memory Access):** Read or write data from/to memory.
5.  **WB (Write Back):** Write the result back to a register.

In a non-pipelined CPU, one instruction must complete all five stages before the next one can begin. In a pipelined CPU, a new instruction can start the first stage as soon as the previous instruction has moved to the second stage. This creates an assembly-line effect where multiple instructions are in different stages of execution at the same time within the same CPU core.

The key difference from parallelism is that pipelining overlaps stages of sequential instructions within *one processor*, while parallelism executes different instructions simultaneously on *multiple processors*.

##### 1.6 Performance via Speculation (Prediction)
**Speculative Execution** is an optimization technique where a computer system performs a task before it is known whether the work is actually needed. The goal is to prevent the CPU from sitting idle while waiting for the outcome of a preceding instruction, such as a conditional branch (`if-else` or `switch` statement).

The CPU's **branch predictor** tries to guess which path of execution will be taken. For example, in a `switch` statement, the processor might predict which `case` is most likely to be executed. It will then *speculatively* start executing the instructions for that predicted path.

*   If the prediction was correct, the results are kept, and time has been saved.
*   If the prediction was wrong, the results of the speculative execution are discarded, and the CPU starts over on the correct path. Although there's a penalty for a wrong guess, on average, the time saved from correct predictions leads to a significant performance boost.

##### 1.7 Dependability via Redundancy
**Redundancy** is the principle of including duplicate or "spare" components in a system to increase its **dependability** and **reliability**. If a primary component fails, a backup component can take over its function, ensuring the system continues to operate without interruption.

This approach is critical in fault-tolerant systems where failure is not an option, such as in spacecraft, aircraft control systems, or critical financial servers. For instance, a spacecraft might be designed with three identical CPUs all performing the same calculations. The results are compared, and if one CPU produces a different result, it is assumed to be faulty and is ignored by the system.

##### 1.8 Make the Common Case Fast
This is a fundamental design principle that states that to improve overall performance, you should focus your optimization efforts on the parts of the system that are used most frequently. The benefit gained from making a rare operation faster is often negligible, but a small improvement in a very common operation can have a huge impact.

For example, if a program spends 90% of its time executing a particular function, then doubling the speed of that function will nearly double the speed of the entire program. In contrast, doubling the speed of a function that only accounts for 1% of the execution time will have almost no noticeable effect on overall performance. This principle guides architects and programmers on where to invest their time and resources for the greatest return.

##### 1.9 Finite State Machines (FSM)
A **Finite State Machine (FSM)** is a mathematical model of computation used to design and describe the behavior of systems. An FSM consists of a *finite* number of **states** and a set of **transitions** that define how the system moves from one state to another in response to specific inputs or events.

At any given moment, the system can only be in one state. For example, a vending machine can be modeled as an FSM:

*   **States:** `Awaiting Client`, `Requesting Payment`, `Delivering Product`, `Returning Money`.
*   **Transitions:** A client choosing a product transitions the machine from `Awaiting Client` to `Requesting Payment`. A successful payment transitions it to `Delivering Product`.

FSMs are widely used in computer architecture to model the behavior of digital logic circuits, processor control units, and communication protocols.

#### 2. Definitions

*   **Computer Architecture:** The set of rules and methods that describe the functionality, organization, and implementation of computer systems. It acts as the blueprint for designing the system.
*   **Hierarchy of Memories:** A tiered structure of computer storage where memory is organized based on access speed, capacity, and cost. Faster tiers are smaller and more expensive.
*   **Cache:** A small, high-speed volatile memory that provides high-speed data access to a processor and stores frequently used computer programs, applications, and data.
*   **CPU Registers:** A small set of data holding places that are part of the computer processor. A register may hold an instruction, a storage address, or any kind of data.
*   **Moore's Law:** The observation that the number of transistors in an integrated circuit doubles about every two years.
*   **Parallelism:** The simultaneous execution of multiple instructions or tasks on different processors or cores to increase computational speed.
*   **Pipelining:** A technique where multiple instructions are overlapped in execution within a single processor. Each instruction is broken into stages, and different instructions can be in different stages at the same time.
*   **Speculative Execution:** An optimization technique where a processor executes instructions before it is certain they will be needed, based on a prediction of program flow.
*   **Redundancy:** The inclusion of extra components that are not strictly necessary to functioning, in case of failure in other components.
*   **Finite State Machine (FSM):** An abstract model of computation that can be in exactly one of a finite number of states at any given time.

#### 3. Mistakes

*   **Confusing Parallelism and Pipelining:** Thinking these are the same concept. **Why it's wrong:** Parallelism involves using multiple, independent processing units to execute different tasks simultaneously. Pipelining is a technique used within a single processing unit to overlap the stages of sequential instructions. It improves throughput, but does not execute instructions truly simultaneously in the way parallelism does.
*   **Believing Moore's Law is about CPU speed:** Stating that Moore's Law dictates that CPU clock speeds double every two years. **Why it's wrong:** Moore's Law is specifically about the number of transistors on a chip. While this historically led to faster speeds, the direct correlation has broken down due to heat and power limitations. The law itself is about density, not speed.
*   **Assuming more cores directly translates to proportional speedup:** Thinking that a program will run four times faster on a quad-core CPU than on a single-core CPU. **Why it's wrong:** This is only true for tasks that are "embarrassingly parallel" (can be perfectly split into independent parts). Most programs have sequential parts and dependencies that cannot be parallelized, limiting the overall speedup (a concept described by Amdahl's Law).
*   **Ignoring the "Common Case Fast" principle:** Spending significant time optimizing a part of the code that is rarely executed. **Why it's wrong:** The overall performance gain is a product of how much faster a part is and how often it is used. Optimizing an infrequently used component yields a negligible improvement to the total execution time, making it an inefficient use of development resources.
*   **Treating all memory as equal:** Writing code that accesses memory in random patterns without considering the memory hierarchy. **Why it's wrong:** Accessing data that is close together in memory (spatial locality) allows the cache to work efficiently, dramatically speeding up execution. Random memory access patterns lead to frequent cache misses, forcing the CPU to wait for the much slower main memory and crippling performance.

#### 4. Examples

##### Example 1: Hierarchy of Memories
**Question:** A programmer declares an integer variable `x = 10` inside a function that performs a tight loop of calculations with it. After the program finishes, the result is saved to a file on the hard drive. Where would the value of `x` most likely reside during each phase: (a) active calculation in the loop, (b) after the program closes, and (c) while the function is running but `x` is not the immediate operand?

<details>
<summary>Click to see the solution</summary>

1.  **Analyze Phase (a):** During a tight loop of active calculations, the CPU needs the fastest possible access to the variable. This is the primary use case for registers.
2.  **Analyze Phase (b):** After the program closes, all volatile memory (registers, cache, RAM) is cleared. The only place the result persists is in non-volatile storage.
3.  **Analyze Phase (c):** While the function is running, the variable `x` is loaded from storage into system RAM. If it's used frequently, it will be copied to the cache for faster access, even when it's not the immediate operand being processed by the ALU.

**Answer:**
(a) In a **CPU Register**.
(b) In **Storage** (the hard drive/SSD).
(c) In **Cache Memory** or **System Memory (RAM)**.
</details>

##### Example 2: Parallelism vs. Pipelining
**Question:** A fast-food restaurant needs to speed up its order processing. It considers two options:
1.  Hire three more cashiers to run three additional checkout counters. Each cashier handles an entire order from start to finish (take order, process payment, get food).
2.  Keep one cashier but create an assembly line with three additional workers: one takes the order, the next processes payment, the next bags the food, and the last hands it to the customer.

Which option is an analogy for parallelism, and which is for pipelining? Explain why.

<details>
<summary>Click to see the solution</summary>

1.  **Analyze Option 1:** This option involves multiple independent units (cashiers) performing the same complete task on different inputs (customers) simultaneously. This is the definition of parallelism.
2.  **Analyze Option 2:** This option involves breaking a single task (processing one order) into a series of sequential stages. Multiple orders can be in different stages of the process at the same time, increasing the overall throughput of the single checkout counter. This is the definition of pipelining.

**Answer:**
Option 1 (multiple cashiers) is an analogy for **parallelism**.
Option 2 (the assembly line) is an analogy for **pipelining**.
</details>

##### Example 3: Make the Common Case Fast
**Question:** You are profiling a web server application and find that 80% of its execution time is spent on a single database query function, `getUserProfile()`. The rest of the time is split among 50 other minor functions. You have one week to optimize the application. Should you focus on improving `getUserProfile()` or on optimizing several of the other minor functions?

<details>
<summary>Click to see the solution</summary>

1.  **Identify the Common Case:** The profiling data clearly shows that the `getUserProfile()` function is the "common case" in terms of time consumption, accounting for 80% of the workload.
2.  **Apply the Principle:** According to the "Make the Common Case Fast" principle, the largest performance gains will come from optimizing the most frequently executed or most time-consuming part of the system.
3.  **Evaluate the Impact:** Even a small 10% improvement in `getUserProfile()` would reduce the total runtime by 8% (10% of 80%). Achieving a similar 8% total reduction by optimizing the other functions would require a much larger and more difficult optimization effort across many different parts of the code.

**Answer:** You should focus exclusively on improving the **`getUserProfile()`** function.
</details>

##### Example 4: Finite State Machine
**Question:** Design a simple FSM for a pedestrian crosswalk button. The system has two states: `Red Light` (cars can go) and `Green Light` (pedestrians can cross). The system has two inputs: a `button_press` event and a `timer_expires` event. Describe the states and the transitions between them.

<details>
<summary>Click to see the solution</summary>

1.  **Define States:** The problem specifies the two states:
    *   `Red Light` (Initial State): Traffic light is red for pedestrians.
    *   `Green Light`: Traffic light is green for pedestrians.
2.  **Define Transitions from `Red Light`:**
    *   What happens if the button is pressed? The system should transition to the `Green Light` state to let the pedestrian cross.
    *   What happens if the timer expires? Nothing, the light stays red for pedestrians until the button is pressed.
3.  **Define Transitions from `Green Light`:**
    *   What happens if the button is pressed? Nothing, the light is already green.
    *   What happens if the timer expires? The pedestrian's crossing time is over, so the system must transition back to the `Red Light` state to let cars go.

**Answer:**
*   **States:** `Red Light` (initial), `Green Light`.
*   **Transitions:**
    *   From `Red Light`, on `button_press` -> transition to `Green Light` and start a timer.
    *   From `Green Light`, on `timer_expires` -> transition to `Red Light`.
</details>