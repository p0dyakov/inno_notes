---
title: "16. Vector Spaces, Axioms, Linearity, and Transformations"
author: "Zakhar Podyakov"
date: "September 18, 2025"
format: html
engine: knitr
---

{{< video https://www.youtube.com/watch?v=TgKwz5Ikpc8&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=17 >}}

<!-- [QUIZ]() | [FLASHCARDS]() -->

#### **1. Summary**
##### **1.1 The Nature of Mathematical Definitions**
Mathematics often uses abstraction to create powerful, generalized tools. As Vladimir Arnold noted, some formal definitions and axioms can seem unmotivated and serve to make a subject difficult for newcomers. However, this abstractness is the price of generality. By defining concepts like "vectors" and "linearity" through a set of fundamental rules (axioms), the resulting theories and tools can be applied to a wide variety of seemingly different contexts, from geometric arrows to functions in calculus.

##### **1.2 What is a Vector? Three Perspectives**
The fundamental question "What are vectors?" can be answered from three different viewpoints, each valid in its own context.

###### **1.2.1 The Physicist's Perspective: Geometric Vectors**
From a physics or geometry standpoint, a **vector** is an arrow in space defined by its length (magnitude) and direction. It is a fundamental object that can be described with a list of numbers (coordinates) for convenience, but its primary identity is spatial.

###### **1.2.2 The Programmer's Perspective: Algebraic Vectors**
From a computer science or programming perspective, a **vector** is fundamentally a list of numbers. This list can be visualized as an arrow in space, but its core identity is the ordered collection of its components. This view is straightforward and unambiguous, easily extending to higher dimensions (e.g., a 4D vector is a list of 4 numbers, a 100D vector is a list of 100 numbers) where geometric visualization is impossible.

###### **1.2.3 The Mathematician's Perspective: Vector Spaces**
A mathematician's answer is that a **vector** is anything that belongs to a **vector space**. This is a more abstract definition. It encompasses both arrows and lists of numbers, but also includes other objects, like functions. This abstraction allows the powerful tools of linear algebra to be applied far beyond simple geometry.

##### **1.3 Vector Spaces**
A **vector space** is a collection of objects (which we call "vectors") that follow a specific set of rules, or **axioms**. The only two things you need to be able to do with these objects are:
1.  Add any two vectors together.
2.  Multiply any vector by a scalar (a number).

If these operations of addition and scaling follow eight fundamental rules (the axioms), the collection is a valid vector space.

###### **1.3.1 Functions as Vectors**
Functions can be treated as vectors. Just as you can add two vectors, you can add two functions, `f(x)` and `g(x)`, to get a new function, `(f + g)(x)`.
<!-- DIAGRAM HERE -->
Similarly, you can scale a function `f(x)` by a scalar `c` to get a new function `(c*f)(x)`. These operations of adding and scaling functions behave in the same sensible way that adding and scaling geometric vectors do, satisfying the required axioms.

##### **1.4 The Eight Axioms of a Vector Space**
These are the formal rules that an operation of addition and scalar multiplication must satisfy for a collection of objects to be considered a **vector space**.
1.  Associativity of addition: $\vec{u} + (\vec{v} + \vec{w}) = (\vec{u} + \vec{v}) + \vec{w}$
2.  Commutativity of addition: $\vec{v} + \vec{w} = \vec{w} + \vec{v}$
3.  Existence of a zero vector: There is a vector $\vec{0}$ such that $\vec{0} + \vec{v} = \vec{v}$ for all $\vec{v}$.
4.  Existence of an additive inverse: For every vector $\vec{v}$, there is a vector $-\vec{v}$ such that $\vec{v} + (-\vec{v}) = \vec{0}$.
5.  Associativity of scalar multiplication: $a(b\vec{v}) = (ab)\vec{v}$
6.  Existence of a scalar identity: The scalar `1` times a vector $\vec{v}$ equals $\vec{v}$.
7.  Distributivity over vector addition: $a(\vec{v} + \vec{w}) = a\vec{v} + a\vec{w}$
8.  Distributivity over scalar addition: $(a+b)\vec{v} = a\vec{v} + b\vec{v}$

These axioms act as an **interface**. A mathematician can prove theorems based only on these axioms, and those theorems will then automatically apply to *any* vector space, whether it's made of arrows, lists of numbers, or functions.

##### **1.5 Linearity and Transformations in Function Spaces**
The concept of a **linear transformation** also extends from geometric vectors to abstract vector spaces like those containing functions.

###### **1.5.1 The Derivative as a Linear Operator**
The derivative from calculus is a perfect example of a **linear transformation** (often called a **linear operator** when dealing with functions). It takes a function as input and produces another function (its derivative) as output. The derivative is linear because it satisfies the two core properties:
1.  **Additivity**: The derivative of a sum of functions is the sum of their derivatives.
    $$ \frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x) $$
2.  **Scaling**: The derivative of a function multiplied by a constant is that constant times the derivative of the function.
     $$ \frac{d}{dx}(c \cdot f(x)) = c \frac{d}{dx}f(x) $$

###### **1.5.2 Representing the Derivative with a Matrix**
To describe this transformation with a matrix, we first need a coordinate system for our function space. Let's consider the vector space of all polynomials.
- **Basis Functions**: A natural choice for a basis is the set of pure powers of $x$: $b_0(x)=1$, $b_1(x)=x$, $b_2(x)=x^2$, and so on. This is an infinite basis, making our vector space **infinite-dimensional**.
- **Polynomials as Vectors**: Any polynomial can be represented by its coefficients in this basis. For example, $P(x) = 5 \cdot 1 + 4x + 1x^2 + 1x^3$ corresponds to the infinite coordinate vector $\begin{pmatrix} 5, 4, 1, 1, 0, \dots \end{pmatrix}^T$.

The matrix for the derivative is found by seeing where it sends each basis function. The $n$-th column of the matrix is the coordinate vector of the derivative of the $n$-th basis function.
- $ \frac{d}{dx}(b_0) = \frac{d}{dx}(1) = 0 $, which has coordinates $\begin{pmatrix} 0, 0, 0, \dots \end{pmatrix}^T$.
- $ \frac{d}{dx}(b_1) = \frac{d}{dx}(x) = 1 $, which has coordinates $\begin{pmatrix} 1, 0, 0, \dots \end{pmatrix}^T$.
- $ \frac{d}{dx}(b_2) = \frac{d}{dx}(x^2) = 2x $, which has coordinates $\begin{pmatrix} 0, 2, 0, \dots \end{pmatrix}^T$.
- $ \frac{d}{dx}(b_n) = \frac{d}{dx}(x^n) = nx^{n-1} $, which has coordinates with $n$ in the $(n-1)$-th position.
This results in an infinite matrix:
$$
\frac{d}{dx} = 
\begin{pmatrix}
0 & 1 & 0 & 0 & \cdots \\
0 & 0 & 2 & 0 & \cdots \\
0 & 0 & 0 & 3 & \cdots \\
0 & 0 & 0 & 0 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}
$$

#### **2. Definitions**
* **Vector**: An object that is an element of a vector space. It can be represented in many forms, such as an arrow in space, a list of numbers, or a function, as long as it adheres to the axioms of vector spaces.
* **Vector Space**: A collection of objects called vectors, along with a field of scalars, where two operations—vector addition and scalar multiplication—are defined and satisfy eight specific axioms.
* **Axioms**: A set of fundamental rules or statements that are assumed to be true and serve as the starting point for deducing other truths in a logical system.
* **Linear Transformation (Operator)**: A function between two vector spaces, $L: V \to W$, that preserves the operations of vector addition and scalar multiplication. This means for any vectors $\vec{u}, \vec{v} \in V$ and any scalar $c$, it satisfies $L(\vec{u}+\vec{v}) = L(\vec{u}) + L(\vec{v})$ and $L(c\vec{v}) = cL(\vec{v})$. When applied to function spaces, it is often called a linear operator.
* **Eigenfunction**: A non-zero function that, when a linear operator is applied to it, results in a scalar multiple of itself. It is the function-space analog of an eigenvector. The scalar multiplier is the corresponding eigenvalue.
* **Inner Product**: A generalization of the dot product for abstract vector spaces. It takes two vectors and returns a scalar, providing a notion of angle and length.

#### **3. Formulas**
* **Additivity of Linear Transformations**:
  $$ L(\vec{v} + \vec{w}) = L(\vec{v}) + L(\vec{w}) $$
* **Scaling Property of Linear Transformations**:
  $$ L(c\vec{v}) = cL(\vec{v}) $$
* **Derivative Sum Rule (Additivity)**:
  $$ \frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x) $$
* **Derivative Constant Multiple Rule (Scaling)**:
  $$ \frac{d}{dx}(c \cdot f(x)) = c \frac{d}{dx}f(x) $$
* **Eigenvector / Eigenfunction Equation**:
  $$ A\vec{v} = \lambda\vec{v} $$
  Here, $A$ is the linear operator, $\vec{v}$ is the eigenvector (or eigenfunction), and $\lambda$ is the scalar eigenvalue.

#### **4. Mistakes**
* **Assuming Vectors are Only Arrows in Space:** This is a common starting point but is too restrictive. Many other mathematical objects, like polynomials and functions, behave like vectors and form vector spaces.
* **Believing All Vector Spaces are 2D or 3D:** While 2D and 3D spaces are excellent for visualization, linear algebra applies to spaces of any number of dimensions, including infinite dimensions, like the space of all polynomials.
* **Treating Axioms as Unimportant Formalities:** The eight axioms of a vector space are crucial because they define the structure. Any system that satisfies them can be analyzed with the tools of linear algebra, which is why the field is so broadly applicable.
* **Forgetting that Linearity Has a Strict Definition:** The intuitive geometric idea of "grid lines remaining parallel and evenly spaced" is a consequence of the two formal properties of linearity: additivity and scaling. These two properties are the true test of whether a transformation is linear.

#### **5. Examples**
##### **5.1. Identifying a Vector Space**
**Question:** Do all polynomials of degree 3 or less (e.g., $ax^3 + bx^2 + cx + d$) form a vector space?
<details>
<summary>Click to see the solution</summary>

Yes, they do.
1.  **Addition:** Adding two polynomials of degree 3 or less results in another polynomial of degree 3 or less. For example, $(2x^3 + x) + (x^2 - 4) = 2x^3 + x^2 + x - 4$.
2.  **Scalar Multiplication:** Multiplying a polynomial of degree 3 or less by a scalar results in another polynomial of degree 3 or less. For example, $5(x^2 - 2x) = 5x^2 - 10x$.
3.  **Axioms:** All eight axioms hold. For example, the zero vector is the zero polynomial ($0x^3 + 0x^2 + 0x + 0$), and addition is commutative and associative.

**Answer:** Yes, it is a vector space. In fact, it is a 4-dimensional vector space with a basis of $\{x^3, x^2, x, 1\}$.
</details>

##### **5.2. Testing for Linearity**
**Question:** Is the transformation $L(f(x)) = f(x) + 1$ a linear transformation on the space of polynomials?
<details>
<summary>Click to see the solution</summary>

Let's test the two properties of linearity.
1.  **Additivity:** Is $L(f(x) + g(x)) = L(f(x)) + L(g(x))$?
    -   $L(f(x) + g(x)) = (f(x) + g(x)) + 1$
    -   $L(f(x)) + L(g(x)) = (f(x) + 1) + (g(x) + 1) = f(x) + g(x) + 2$
    Since $f(x) + g(x) + 1 \neq f(x) + g(x) + 2$, the additivity property fails.

**Answer:** No, the transformation is not linear.
</details>

##### **5.3. Representing a Polynomial as a Vector**
**Question:** Using the basis $\{1, x, x^2, x^3, \dots\}$, what is the coordinate vector for the polynomial $P(x) = 8x^3 - 6x$?
<details>
<summary>Click to see the solution</summary>
We need to find the coefficients for each basis function: $c_0 \cdot 1 + c_1 \cdot x + c_2 \cdot x^2 + c_3 \cdot x^3 + \dots$
- The coefficient of $1$ is $0$.
- The coefficient of $x$ is $-6$.
- The coefficient of $x^2$ is $0$.
- The coefficient of $x^3$ is $8$.
- All higher-order coefficients are $0$.
The coordinate vector is an infinite column vector listing these coefficients in order.

**Answer:** The coordinate vector is $\begin{pmatrix} 0, -6, 0, 8, 0, 0, \dots \end{pmatrix}^T$.
</details>

##### **5.4. Applying the Derivative Matrix**
**Question:** Use the derivative matrix to find the derivative of $P(x) = 2x^2 + 5x - 3$.
<details>
<summary>Click to see the solution</summary>
1.  **Write the coordinate vector for $P(x)$:** The basis is $\{1, x, x^2, \dots\}$, so the vector is $\vec{p} = \begin{pmatrix} -3, 5, 2, 0, \dots \end{pmatrix}^T$.
2.  **Multiply the derivative matrix by the vector:**
    $$
    \begin{pmatrix}
    0 & 1 & 0 & 0 & \cdots \\
    0 & 0 & 2 & 0 & \cdots \\
    0 & 0 & 0 & 3 & \cdots \\
    \vdots & \vdots & \vdots & \vdots & \ddots
    \end{pmatrix}
    \begin{pmatrix}
    -3 \\ 5 \\ 2 \\ 0 \\ \vdots
    \end{pmatrix}
    =
    \begin{pmatrix}
    (0 \cdot -3) + (1 \cdot 5) + (0 \cdot 2) + \dots \\
    (0 \cdot -3) + (0 \cdot 5) + (2 \cdot 2) + \dots \\
    (0 \cdot -3) + (0 \cdot 5) + (0 \cdot 2) + \dots \\
    \vdots
    \end{pmatrix}
    =
    \begin{pmatrix}
    5 \\ 4 \\ 0 \\ 0 \\ \vdots
    \end{pmatrix}
    $$
3.  **Convert the resulting vector back to a polynomial:** The vector $\begin{pmatrix} 5, 4, 0, \dots \end{pmatrix}^T$ corresponds to $5 \cdot 1 + 4 \cdot x = 4x + 5$.

**Answer:** The derivative is $4x + 5$.
</details>

##### **5.5. Eigenfunctions of the Derivative**
**Question:** Is the function $f(x) = e^{2x}$ an eigenfunction of the derivative operator $\frac{d}{dx}$? If so, what is its eigenvalue?
<details>
<summary>Click to see the solution</summary>
An eigenfunction must satisfy the equation $A\vec{v} = \lambda\vec{v}$. Here, the operator $A$ is $\frac{d}{dx}$ and the vector $\vec{v}$ is the function $f(x) = e^{2x}$.
1.  **Apply the operator:**
    $$ \frac{d}{dx}(e^{2x}) = 2e^{2x} $$
2.  **Compare the result to $\lambda f(x)$:**
    The result is $2 \cdot (e^{2x})$, which is in the form $\lambda \cdot f(x)$.

**Answer:** Yes, $e^{2x}$ is an eigenfunction of the derivative operator with an eigenvalue of $\lambda = 2$.
</details>

##### **5.6. Another Vector Space**
**Question:** Consider the set of all $2 \times 2$ matrices. Can this be a vector space?
<details>
<summary>Click to see the solution</summary>
Yes. We can define addition and scalar multiplication for matrices.
1.  **Addition:** Two $2 \times 2$ matrices can be added component-wise to produce another $2 \times 2$ matrix.
2.  **Scalar Multiplication:** A $2 \times 2$ matrix can be multiplied by a scalar, resulting in another $2 \times 2$ matrix.
3.  **Axioms:** These operations satisfy all eight axioms. The "zero vector" is the $2 \times 2$ zero matrix: $\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$.

**Answer:** Yes, the set of all $2 \times 2$ matrices forms a vector space.
</details>

##### **5.7. A Non-Linear Operator**
**Question:** Is the transformation $L(f(x)) = (f(x))^2$ a linear operator?
<details>
<summary>Click to see the solution</summary>
Let's test the scaling property. Is $L(c \cdot f(x)) = c \cdot L(f(x))$?
1.  **Apply the operator to the scaled function:**
    $$ L(c \cdot f(x)) = (c \cdot f(x))^2 = c^2 (f(x))^2 $$
2.  **Scale the result of the operator:**
    $$ c \cdot L(f(x)) = c \cdot (f(x))^2 $$
Since $c^2 (f(x))^2 \neq c (f(x))^2$ (unless $c=0$ or $c=1$), the scaling property fails.

**Answer:** No, this is not a linear operator.
</details>