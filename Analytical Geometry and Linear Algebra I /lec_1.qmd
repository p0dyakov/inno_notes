---
title: "1. Vectors, Vector Spaces, and Linear Independence"
author: "Zakhar Podyakov"
date: "September 16, 2025"
format: html
---

#### 1. Summary

##### 1.1 What is a Vector?
A **vector** is a fundamental mathematical object that possesses both *magnitude* (or length) and *direction*. Vectors can be understood in two primary ways:

*   **Geometrically:** A vector is represented as a directed line segment or an arrow in space. The length of the arrow corresponds to its magnitude, and the direction it points to is its direction. A key property is that a vector is independent of its starting position; it is defined only by its length and orientation.
*   **Algebraically:** A vector is an ordered list of numbers, called **components** or **coordinates**. This list can be written as a column or a row. For instance, a vector in a 2D plane can be written as $\vec{v} = \begin{pmatrix} x \\ y \end{pmatrix}$ or $\vec{v} = (x, y)$.

There is a one-to-one correspondence between a point $P(x, y, z)$ in space and its **position vector** $\vec{OP}$, which starts at the origin $O(0, 0, 0)$ and ends at point $P$.

##### 1.2 Vector Operations
Standard operations can be performed on vectors:

*   **Vector Addition:** To add two vectors, $\vec{u}$ and $\vec{v}$, you add their corresponding components. Geometrically, this is visualized using the *tip-to-tail method*: place the tail of $\vec{v}$ at the tip of $\vec{u}$. The resulting vector, $\vec{u} + \vec{v}$, starts at the tail of $\vec{u}$ and ends at the tip of $\vec{v}$.
*   **Scalar Multiplication:** To multiply a vector $\vec{v}$ by a **scalar** (a single number) $c$, you multiply each component of $\vec{v}$ by $c$. This operation *scales* the vector's magnitude. If $c > 0$, the direction remains the same. If $c < 0$, the direction is reversed.
*   **Vector Subtraction:** Subtracting $\vec{v}$ from $\vec{u}$ is equivalent to adding the negative of $\vec{v}$: $\vec{u} - \vec{v} = \vec{u} + (-1)\vec{v}$.

##### 1.3 Vector Norm and Distance
The **norm**, or magnitude, of a vector $\vec{v}$, denoted as $||\vec{v}||$, represents its length. It is calculated by taking the square root of the sum of the squares of its components. This is a generalization of the Pythagorean theorem.

The distance between two points, $P$ and $Q$, in space is simply the norm of the vector connecting them, $\vec{PQ}$.
$$ d(P, Q) = ||\vec{PQ}|| = ||Q - P|| $$

A **unit vector** is a vector with a norm of 1. Any non-zero vector can be converted into a unit vector pointing in the same direction through a process called **normalization**: dividing the vector by its own norm.

##### 1.4 Vector Spaces and Subspaces
A **vector space** is a collection of objects (vectors) that can be added together and multiplied by scalars, adhering to a set of ten rules, or *axioms*. These axioms ensure that vector operations behave consistently and predictably. The set of all 2D vectors, denoted $\mathbb{R}^2$, is a classic example of a vector space.

A **subspace** is a subset of a larger vector space that is, by itself, also a vector space. To verify if a subset $H$ is a subspace, we use the **Subspace Test**, which simplifies the process to checking just three conditions:
1.  The **zero vector** of the parent space must be in $H$.
2.  $H$ must be **closed under addition** (if $\vec{u}, \vec{v} \in H$, then $\vec{u}+\vec{v} \in H$).
3.  $H$ must be **closed under scalar multiplication** (if $\vec{u} \in H$ and $c$ is a scalar, then $c\vec{u} \in H$).

##### 1.5 Linear Combinations, Span, Basis, and Dimension
*   A **linear combination** of a set of vectors is a new vector formed by summing scalar multiples of those vectors.
*   The **span** of a set of vectors is the set of *all possible* linear combinations that can be formed from them. The span of a set of vectors always forms a subspace. For example, the span of two non-collinear vectors in $\mathbb{R}^3$ is a plane passing through the origin.
*   A set of vectors is **linearly independent** if no vector in the set can be expressed as a linear combination of the others. This is the ideal case, containing no redundant vectors. The only way to sum their scalar multiples to get the zero vector is if all scalars are zero.
*   A set of vectors is **linearly dependent** if at least one vector is redundant (i.e., it lies within the span of the others).
*   A **basis** for a vector space is a set of linearly independent vectors that spans the entire space. It is the smallest set of vectors needed to "build" the whole space.
*   The **dimension** of a vector space is the number of vectors in its basis.

##### 1.6 Vector Projections
A vector can be broken down into components relative to another vector. The **vector projection** of $\vec{a}$ onto $\vec{b}$ (denoted $\vec{a'}$) is the "shadow" that $\vec{a}$ casts on the line defined by $\vec{b}$. It represents the component of $\vec{a}$ that is parallel to $\vec{b}$. The other component, the **vector rejection** ($\vec{a''}$), is perpendicular to $\vec{b}$ and is found by $\vec{a''} = \vec{a} - \vec{a'}$.

#### 2. Definitions

*   **Vector**: A mathematical object possessing both magnitude (length) and direction, typically represented by an ordered list of numbers.
*   **Scalar**: A single numerical quantity used to scale vectors.
*   **Norm (Magnitude)**: The length of a vector, calculated as the square root of the sum of the squares of its components.
*   **Unit Vector**: A vector with a norm of 1.
*   **Vector Space**: A set of vectors and a set of scalars that satisfy ten axioms governing addition and scalar multiplication.
*   **Subspace**: A subset of a vector space that is itself a vector space under the same operations.
*   **Linear Combination**: A sum of vectors, each multiplied by a scalar coefficient.
*   **Span**: The set of all possible linear combinations of a set of vectors.
*   **Linearly Independent**: A set of vectors where no vector can be written as a linear combination of the others.
*   **Linearly Dependent**: A set of vectors where at least one vector can be written as a linear combination of the others.
*   **Basis**: A set of linearly independent vectors that spans an entire vector space.
*   **Dimension**: The number of vectors in a basis for a vector space.
*   **Vector Projection**: The component of one vector that lies in the direction of another vector.

#### 3. Formulas

*   **Vector Addition**: $\vec{u} + \vec{v} = \begin{pmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \end{pmatrix}$
*   **Scalar Multiplication**: $c\vec{v} = \begin{pmatrix} cv_1 \\ cv_2 \\ \vdots \end{pmatrix}$
*   **Vector Norm**: $||\vec{v}|| = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2}$
*   **Distance Formula**: $d(P, Q) = ||\vec{Q} - \vec{P}|| = \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \dots}$
*   **Normalization (Unit Vector)**: $\hat{v} = \frac{\vec{v}}{||\vec{v}||}$
*   **Vector Projection** of $\vec{a}$ onto $\vec{b}$: $\vec{a'} = \text{proj}_{\vec{b}}\vec{a} = \left( \frac{\vec{a} \cdot \vec{b}}{||\vec{b}||^2} \right) \vec{b}$
*   **Vector Rejection** of $\vec{a}$ from $\vec{b}$: $\vec{a''} = \vec{a} - \vec{a'}$

#### 4. Mistakes

*   **Assuming a Set is a Subspace without Verification:** Forgetting to check one of the three conditions (zero vector, closure under addition, closure under scalar multiplication). For instance, a line in $\mathbb{R}^2$ not passing through the origin is not a subspace because it fails the zero vector test.
*   **Adding a Scalar to a Vector:** Operations like $5 + \begin{pmatrix} 2 \\ 3 \end{pmatrix}$ are undefined. **Why it's wrong:** Scalars and vectors are fundamentally different mathematical objects and their operations are defined separately.
*   **Assuming `n` Vectors Form a Basis for $\mathbb{R}^n$:** A set of `n` vectors in an `n`-dimensional space only forms a basis if it both spans the space and is linearly independent. For example, $\left\{ \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 2 \\ 0 \end{pmatrix} \right\}$ are two vectors in $\mathbb{R}^2$, but they do not form a basis because they are linearly dependent and only span a line.
*   **Failing to Check for Closure with Negative Scalars:** The first quadrant $S = \{(x, y) \in \mathbb{R}^2 | x \ge 0, y \ge 0\}$ is not a vector space because it is not closed under multiplication by negative scalars. A vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ is in $S$, but $(-1) \cdot \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} -1 \\ -1 \end{pmatrix}$ is not.

#### 5. Examples

##### Example 1: Finding Unknown Coordinates

**Question:** Determine the unknown coordinates for the points $A(3, 5)$, $B(4, 6)$, $C(-2, 5)$, and $D(x, y)$, given that the vector $\vec{AB}$ is equivalent to the vector $\vec{CD}$.

<details>
<summary>Click to see the solution</summary>

1.  **Define the vectors:** A vector from point P to Q is found by subtracting the coordinates of P from Q.
    $$ \vec{AB} = B - A = (4 - 3, 6 - 5) = (1, 1) $$
    $$ \vec{CD} = D - C = (x - (-2), y - 5) = (x + 2, y - 5) $$
2.  **Set the vectors equal:** For the vectors to be equivalent, their corresponding components must be equal.
    $$ 1 = x + 2 $$
    $$ 1 = y - 5 $$
3.  **Solve for x and y:**
    $$ x = 1 - 2 = -1 $$
    $$ y = 1 + 5 = 6 $$

**Answer:** The coordinates of point D are **$(-1, 6)$**.
</details>

##### Example 2: Linear Dependence

**Question:** Show that the vectors $\vec{u} = (1, 2)$, $\vec{v} = (3, 6)$ are linearly dependent.

<details>
<summary>Click to see the solution</summary>

1.  **Definition of Linear Dependence:** Two vectors are linearly dependent if one is a scalar multiple of the other. We need to find a scalar $c$ such that $\vec{v} = c\vec{u}$.
    $$ (3, 6) = c(1, 2) $$
2.  **Set up component equations:**
    $$ 3 = c \cdot 1 $$
    $$ 6 = c \cdot 2 $$
3.  **Solve for c:** Both equations yield $c=3$. Since a single scalar $c$ exists that satisfies the condition, the vectors are scalar multiples of each other.

**Answer:** The vectors are linearly dependent because **$\vec{v} = 3\vec{u}$**.
</details>

##### Example 3: Checking for a Subspace

**Question:** The set $S = \{ \begin{pmatrix} x \\ y \end{pmatrix} \in \mathbb{R}^2 | x \ge 0, y \ge 0 \}$ represents the first quadrant. Show that $S$ is not a vector space.

<details>
<summary>Click to see the solution</summary>

1.  **Recall the Subspace Test:** A set is a vector space (or subspace) if it contains the zero vector and is closed under both vector addition and scalar multiplication. We only need to show it fails one of these tests.
2.  **Test closure under scalar multiplication:** Pick a vector that is clearly in $S$, for example, $\vec{v} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$. Both components are $\ge 0$.
3.  **Multiply by a negative scalar:** Choose a negative scalar, like $c = -1$.
    $$ c\vec{v} = (-1) \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} -1 \\ -1 \end{pmatrix} $$
4.  **Check if the result is in S:** The resulting vector $\begin{pmatrix} -1 \\ -1 \end{pmatrix}$ has components that are not $\ge 0$. Therefore, it is not in $S$.

**Answer:** The set $S$ is not a vector space because it is **not closed under scalar multiplication**.
</details>

##### Example 4: Vector Projection

**Question:** Consider the vectors $\vec{a} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$ and $\vec{b} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$. Compute the vector projection of $\vec{a}$ onto $\vec{b}$ (the vector $\vec{a'}$).

<details>
<summary>Click to see the solution</summary>

1.  **Recall the projection formula:**
    $$ \vec{a'} = \left( \frac{\vec{a} \cdot \vec{b}}{||\vec{b}||^2} \right) \vec{b} $$
2.  **Calculate the dot product** $\vec{a} \cdot \vec{b}$:
    $$ \vec{a} \cdot \vec{b} = (3)(1) + (4)(0) = 3 $$
3.  **Calculate the squared norm** of $\vec{b}$:
    $$ ||\vec{b}||^2 = 1^2 + 0^2 = 1 $$
4.  **Substitute the values into the formula:**
    $$ \vec{a'} = \left( \frac{3}{1} \right) \begin{pmatrix} 1 \\ 0 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 3 \\ 0 \end{pmatrix} $$
    
This result makes intuitive sense: the "shadow" of the vector $(3, 4)$ on the x-axis is a vector of length 3 along that axis.

```{=tex}
\begin{center}
\begin{tikzpicture}[scale=1.5,
  vec/.style={->, thick, >=stealth}]
  % Axes
  \draw[->, gray] (-0.5,0) -- (4,0) node[right, black] {$x$};
  \draw[->, gray] (0,-0.5) -- (0,5) node[above, black] {$y$};
  
  % Vector b
  \draw[vec, blue] (0,0) -- (1,0) node[midway, below] {$\vec{b}$};
  
  % Vector a
  \draw[vec, red] (0,0) -- (3,4) node[midway, above left] {$\vec{a}$};
  
  % Dashed line for projection
  \draw[dashed, gray] (3,4) -- (3,0);
  
  % Vector a' (projection)
  \draw[vec, teal] (0,0) -- (3,0) node[midway, below] {$\vec{a'}$};
  
  % Right angle symbol
  \draw (3,0.2) -- (2.8,0.2) -- (2.8,0);
\end{tikzpicture}
\end{center}
```

**Answer:** The projection of $\vec{a}$ onto $\vec{b}$ is **$\vec{a'} = \begin{pmatrix} 3 \\ 0 \end{pmatrix}$**.
</details>